{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "#import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_yaml\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_file_path):\n",
    "    '''\n",
    "        Purpose: Function that loads the data from the given path\n",
    "        Arguements: Path to source\n",
    "        Return: raw data containing images and labels\n",
    "    '''\n",
    "    with open(data_file_path) as fp:\n",
    "        data = fp.readlines()\n",
    "    print(\"Labels for the dataset are: {}\".format(data[0].split(\",\")))\n",
    "    data = np.array(data[1:])\n",
    "    print(\"Number of images read: {}\".format(data.size))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_split(data, num_of_classes=7):\n",
    "    '''\n",
    "        Purpose: Function that takes the given dataset and creates Train and Test sets\n",
    "        Arguements: Raw data and number of classes (Categorical label generation)\n",
    "        Return: Training and Test set - inputs and labels\n",
    "    '''\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for point in data:\n",
    "        try:\n",
    "            emotion, image, instance_type = point.split(\",\")\n",
    "            image_pixels = np.array(image.split(\" \"),'float32')\n",
    "            # One hot encoding\n",
    "            emotion = to_categorical(emotion, num_of_classes)\n",
    "            if(instance_type.strip() == 'Training'):\n",
    "                X_train.append(image_pixels)\n",
    "                Y_train.append(emotion)\n",
    "            else:\n",
    "                X_test.append(image_pixels)\n",
    "                Y_test.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR - {}\".format(e))\n",
    "            \n",
    "    X_train = np.array(X_train,'float32')\n",
    "    Y_train = np.array(Y_train,'float32')\n",
    "    X_test = np.array(X_test,'float32')\n",
    "    Y_test = np.array(Y_test,'float32')\n",
    "    return (X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_test):\n",
    "    \n",
    "    '''\n",
    "        Purpose: Function that performs pre-processing on train and test data\n",
    "        Arguements: Train and Test input sets\n",
    "        Return: Processed Train and Test sets\n",
    "    '''\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X_train = X_train / 255\n",
    "    X_test = X_test / 255\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 48, 48, 1).astype('float32')\n",
    "    X_test = X_test.reshape(X_test.shape[0], 48, 48, 1).astype('float32')\n",
    "    \n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(X_train, Y_train, force=False):\n",
    "    if(force or not os.path.exists('model_cnn.yaml')):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(filters=64 , kernel_size=(5,5), strides=(1, 1), padding='valid', dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, input_shape=(48, 48, 1)))\n",
    "        model.add(MaxPooling2D(pool_size=(5, 5), strides=(2,2), padding='valid', data_format=None))\n",
    "\n",
    "        model.add(Conv2D(filters=64 , kernel_size=(3,3), strides=(1, 1), padding='valid', dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, input_shape=(48, 48, 1)))\n",
    "        model.add(Conv2D(filters=64 , kernel_size=(3,3), strides=(1, 1), padding='valid', dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, input_shape=(48, 48, 1)))\n",
    "        model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "        # Fit the model with the training data\n",
    "        model = fit_model(model, X_train, Y_train)\n",
    "    else:\n",
    "        model = load_model()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,X_train, Y_train, batch_size=128 ,epochs=10, loss_function='categorical_crossentropy'):\n",
    "    data_generator = ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)\n",
    "    training_data_generator = data_generator.flow(X_train, Y_train, batch_size=batch_size)\n",
    "    model.compile(loss=loss_function, optimizer=Adam(), metrics=['accuracy'])\n",
    "    model.fit_generator(training_data_generator, steps_per_epoch=batch_size, epochs=epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    train_score = model.evluate(X_train, Y_train, verbose=0)\n",
    "    test_score = model.evluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    print(\"Train loss: {}\".format(train_score[0]))\n",
    "    print(\"Test loss: {}\".format(test_score[0]))\n",
    "    \n",
    "    print(\"Train accuracy: {}\".format(training_score[1]))\n",
    "    print(\"Test accuracy: {}\".format(test_score[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_path):\n",
    "    try:\n",
    "        plt.figure()\n",
    "        image = mpimg.imread(image_path)\n",
    "        #imgplot = plt.imshow(image)\n",
    "        #plt.show()\n",
    "    except:\n",
    "        print(\"ERROR - Unable to read and plot the provided image\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_display(predicted_label):\n",
    "    emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    y_ticks = np.arange(len(emotion_classes))\n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_ticks, emotion_classes)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, image_path):\n",
    "    img = image.load_img(image_path, color_mode='grayscale', target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x /= 255\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    predicted_label = model.predict(x)\n",
    "    print(\"Prediction: {}\".format(predicted_label))\n",
    "    # Draw image provided\n",
    "    show_image(image_path)\n",
    "    # Draw emotion pie chart    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    with open(\"model_cnn.yaml\", \"r\") as yaml_file:\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "    model = model_from_yaml(loaded_model_yaml)\n",
    "    model.load_weights(\"model_cnn.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"model_cnn.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model_cnn.h5\")\n",
    "    print(\"Saved model to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(predictions, Y_test):\n",
    "    predicted_list = []; actual_list = []\n",
    "    for i in predictions:\n",
    "        pred_list.append(np.argmax(i))\n",
    "    for i in Y_test:\n",
    "        actual_list.append(np.argmax(i))\n",
    "    conf_matrix = confusion_matrix(actual_list, predicted_list)\n",
    "    plt.figure()\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    classes = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "        \n",
    "    # Pre-defined variables\n",
    "    data_path = '/Users/Iris/SJSU/Fall_2018/CMPE_257/Project/Group/local/data/fer2013.csv'\n",
    "    \n",
    "    \n",
    "    # Obtain the data from the path provided\n",
    "    data = get_data(data_path)\n",
    "    \n",
    "    # Generate training and test sets from the data\n",
    "    X_train, Y_train, X_test, Y_test = generate_data_split(data)\n",
    "    \n",
    "    # Pre-process the image data\n",
    "    X_train, X_test = preprocessing(X_train,X_test)\n",
    "    \n",
    "    # Generate or load trained model \n",
    "    #model = generate_model(X_train, Y_train)\n",
    "    model = generate_model([],[])\n",
    "    \n",
    "    generate_confusion_matrix(model,predictions, Y_test)\n",
    "    # Evaluate model\n",
    "    #evaluate_model(model, X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    # Save model to disk\n",
    "    #save_model(model)\n",
    "    \n",
    "    # Predict for a given sample\n",
    "    #sample_image_path = 'sample.png'\n",
    "    #prediction(model, sample_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
